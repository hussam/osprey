\documentclass[a4paper,twocolumn]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\newcommand{\ignore}[1]{}

\title{
  It's Never Too Late To Learn\\
  \emph{Using Online Machine Learning in Distributed Systems}
}
\author{}
\date{}

\begin{document}
\maketitle

\section{Motivation}

Building a modern distributed service that guarantees service-level objectives
is hard. Complexity is introduced by the deployment environment and exacerbated
as the service evolves with time to accommodate hardware/environment changes or
support new features.
For example, to build a simple caching layer that guarantees a certain hit rate,
response latency, and throughput, developers need to take into account the
expected workload accessing the cache, the number of machines available to run
it, the network connecting these machines, the storage hierarchy within each
machine, and interference from other applications running on the network and
machines.  A change in any one of these factors could render the service
woefully inefficient.

Traditionally, this complexity is addressed by using highly-skilled developers
to build custom systems that are optimized for specific configurations and
objective functions.  Is there a general, principled, approach for dealing with
complexity in distributed systems? Hand-tuning the design and configuration of a
system does not scale well with increased complexity or changes in software
design, hardware architecture, deployment environment, and service-level
objectives.

We propose to use online learning to cope with this complexity. Specifically, we
view a {\em decision} made by a distributed system in the framework of
contextual online learning with {\em partial feedback} (where the learner only
gets to observe the result of the action it took, rather than all possible
actions). This is a natural fit for distributed systems where: there is plenty
of context surrounding each decision, decisions are made online, and feedback is
received only for decisions that were executed.  Thus, a hand-designed policy in
a distributed systems can be replaced with policy that has been optimized by a
contextual learning system. By supplying rich contexts to this policy,
developers capture the complexity of our environment without having to
understand it. Additionally, by optimizing the policy continuously, the learning
system can swiftly adapt to changes in the environment or service.

%% Hussam: I changed "full information" to "full feedback" to stay consistent
%% with the naming above. Confirmed with Akshay that this is correct terminology

Machine learning has certainly been used in the past to optimize system
decisions. Most of this use has been restricted to either offline settings or
expect {\em full feedback}, whereby a supervised learner is trained on data
that has been annotated with correct labels.
Some systems work in an online setting with full feedback. An example of that
is a caching layer that uses a learning system to dynamically switch between
existing (hand-designed) cache eviction policies, by running all policies in
parallel and measuring their performance.
Systems that work in an online setting with partial feedback
tend to be for news or ad recommendation, but examples from systems
infrastructure exist. For example, caching dynamic query content [], tuning
parameters of multicore data structures [], allocating resources to cluster
computing tasks [], and allocating servers to web applications [].  In all of
these systems, the context used to make a decision, and the decision itself, are
{\em local} to a machine. This is achieved by either devising a decentralized
algorithm in the first place, or giving up on global optimality. We are not
aware of any system that makes ``global'' decisions based on truly distributed
architecture or state.

%% Hussam: I don't know what is this "and yet" referencing. So I am taking it out.
%%
%%And yet we use hand-designed policies to make these decisions all the time, such
%%as for request routing, replica placement, replica selection, cloud resource
%%allocation, failure recovery tasks, and others.

What is it about distributed systems that makes it difficult to apply online
learning at decision points such as request routing, replica placement, cloud
resource allocation, and failure recovery? This proposal identifies fundamental
challenges posed by distributed systems that must be analyzed and addressed in
order to make online learning a standard tool for distributed infrastructure
systems.  These challenges hold even if we assume that the online learning
system is a black-box that delivers optimal decisions with zero latency. In
other words, they are fundamental to the way we design distributed systems.  We
begin by highlighting some of the factors that contribute to these challenges,
and then list the challenges themselves.

\subsection*{Known unknowns}

Systems often make decisions without complete knowledge of the factors that
affect, or could affect, those decisions.  For example, a caching system evicts
items based on what it believes future requests will look like, without actually
knowing the future.  In other cases, incomplete knowledge is due to the absence
of data-computation locality.  For example, a load balancer makes decisions
based on what it thinks is the current load on other machines, e.g. based on
data it collected recently, rather than the actual load at that very instant in
time.

In both of these situations, assumptions are made about the unknowns offline
during \emph{design time}, resulting in different request routing and caching
algorithms. A change to the \emph{run-time} environment could easily render
these choices suboptimal.

\ignore{
Online learning has the potential to make these decisions online, based on current contexts,
and can be nimble in adapting to any changes to the run-time
environment.
}

\subsection*{Unknown unknowns}

In some situations, system designers have no way of knowing deployment details
that might impact a system's performance. For example, a system could be built
for external deployment and used by third-party customers, or it could be
deployed in a multi-tenant environment where co-located services could affect
its performance in unexpected ways. In these cases, it is nearly impossible for
developers to account for all unknown variables at design time.

\ignore{
However, an ML agent is able to make
decisions online based on inputs from the run-time environment.
}
\ignore{
To be clear, ML has been used in some distributed systems. However, it has not
been used in a general way to make online decisions about core distributed
systems implementation details. We advocate that developers should focus on
high-level objectives of their system, and use online ML to make to compute the
specifics (such as policies for request routing, replica placement, or failure
monitoring) at run-time based on observed metrics.
}

\section{Challenges}

As mentioned earlier, we adopt the framework of contextual online learning with
partial feedback.  To separate the systems concerns we are trying to highlight
from machine learning concerns, we abstract the learning system as a black-box
and assume it provides optimal decisions with zero latency. For a given decision
over a discrete action space, the interface to the learning system is as
follows: given a {\em context} that summarizes the state of the world, the
learning system chooses an {\em action} to take, which our system does and later
reports a {\em reward} indicating how good the action was. An example of a
learning system that supports this interface is the Decision
Service~\footnote{http://aka.ms/mwt}. Now consider a load balancer using that
learning system to make its request-routing decisions. The context for each
decision could be load information of each machine in the cluster, the action
could be a specific machine to route the request to, and the reward could be the
latency of processing the request.

Thus we have reduced our concerns to generating an appropriate context,
executing an action (told to us by the learning system), and reporting reward
information.  We believe that fundamental properties of distributed systems make
even these tasks difficult, as described below.

\subsection*{Locality}

Decisions in a distributed system often rely on contextual information that
resides on diverse and physically disparate components. This information may not
be readily available at the point at which the decision is made (e.g., the load
balancer).  As a result, the context supplied to the learning system may be
stale. What are the effects of staleness, and can the learning system cope with
them (e.g., by relying on ''smoothness'' properties of the context)?  Can we
leverage recent hardware or software techniques to reduce or eliminate
staleness?

Another disparity occurs between the place a decision is made (e.g., the load
balancer) and where the resulting action is taken (e.g., the server selected to
process a request). This means that the enforcement of the action cannot be
guaranteed by the decision maker, and indeed it may be altered or reversed.
Contextual learning systems are capable of tolerating downstream logic
that alters the decision, provided certain independence assumptions
aren't violated and rewards are reported accurately. Are there circumstances in
which distributed systems would violate these assumptions enough to affect
performance?

While reward information is also not available at the decision location, this is
the norm in contextual learning settings, where reward information is only
discernible after a delay and often by a separate subsystem. Thus, we do not
expect this to pose a problem. (The faster reward information is collected, the
faster the resulting data point can be used by the learning subsystem.)

\subsection*{Failures}

The disparity in locality above results in a lack of fate sharing between the
distributed system components involved in a decision.  Failures in any of
these components have implications on the inputs to the learning system. For
example, server failures in a load balanced cluster could mean that certain
context information is staler than others or unavailable altogether. How do we
convey this ''lack of information'' to the learning system, and how does it
cope with sporadically incomplete data? Should failures be encoded in the
context somehow?
How do learning
systems cope with data omission or inability to execute decisions due to
failures? Can the ML agent learn from incomplete or potentially corrupt data?
How sensitive should the learner be to transient conditions in the system?

\subsection*{Hierarchy}

Distributed systems are often designed in a hierarchical manner, where
subcomponents monitor and manage others. These components and layers of
indirection simplify reasoning, building, and maintaining the system at
large. However, it is not clear how hierarchy should be handled with respect to
optimizing decisions via contextual learning. In general contextual learning is
composable, in that decisions of subcomponents can be used to influence
higher-level components, downstream components can make decisions influenced by
the decisions of upstream components, and so on.  Should the context for each
decision be limited to the level it is at in the hierarchy, or is it better for
contexts to span multiple (or all) levels? Should decision points roughly mimic
the hierarchy of the system? Consider deep memory hierarchy, for example. One
could imagine that by capturing contextual information across the entire
hierarchy, one could decide at what level to cache a piece of data, instead of
only deciding on the current level and relying on evictions to move between
levels.

\subsection*{Randomization}

Contextual learning uses exploration (controlled randomization) to discover
potentially good strategies. This is the famous explore-exploit trade-off and is
well established to be necessary for proper online optimization. Randomization
can be encapsulated and handled by the learning system, but there are two
situations in which it might emerge to the systems level. First, if multiple
components are making decisions independently, it is important to ensure that
the randomization used by each (e.g. the seed passed to the learning system)
are independent of each other. Second, many distributed systems already use
randomization inherently, such as randomized load balancing or randomized cache
eviction strategies. Can this randomization be leveraged by the learning
system to achieve its exploration goals naturally?

\ignore{
An ML agent can be trained offline with pre-labeled data. However, is this
possible in the context of distributed systems? An online learning systems is
more logical in this context, however, how do we quantify the costs of
exploration vs exploitation of learned strategies?
}

\section{Drawbacks}

The approach we have outlined above has at least two drawbacks. One is that we
are using machine-learned models to capture the complexity of a distributed
systems decision. This means that the resulting optimal policy will be opaque to
us, making it difficult to discover what contextual information matters, for
example.  We are independently pursuing work in cracking models open to see what
can be learned from them, but in general this is not always possible nor is it
reliable to draw conclusion from things like learned weights. In effect, our
position is to accept this opacity as the cost of offloading the complexity of
making the right decision to the learning system.

The second drawback is that our proposal only replaces hand-designed policies at
existing decision points. In other words, machine learning is used as an
alternative policy within an existing design and architecture. Can machine
learning influence the design/architecture of the system itself? Systems like
the Decision Service advocate the former approach, but it is conceivable that in
the future distributed systems will be co-designed based on learning objectives.


%% Scope:
%% The above replaces the policy behind a decision with one backed by an ML learning system.
%% Indeed systems like hte DS were designed to intervene at the decision making point in a
%% slim way. The above outlines the issues which tackled would result in ML based policies,
%% new ones. However, once the above has been mastered, there is the question of whether ML
%% can influence distributed systems design at a higher level. For example, can it suggest
%% different mechanisms for achieving a goal, or perhaps a different design of the system in the first place?

\end{document}
