\documentclass[a4paper,twocolumn]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\newcommand{\ignore}[1]{}

\title{
  It's Never Too Late To Learn\\
  \emph{Using Online Machine Learning in Distributed Systems}
}
\author{}
\date{}

\begin{document}
\maketitle

\section{Motivation}

Building a modern distributed service that guarantees service-level objectives
is hard. Complexity is introduced by the deployment environment and exacerbated
as the service evolves with time to accommodate hardware/environment changes or
support new features.
For example, to build a simple caching layer that guarantees a certain hit rate,
response latency, and throughput, developers need to take into account the expected
workload hitting the cache, the number of machines available to run it, the network
connecting these machines, the storage hierarchy within each machine, and interference
from other applications running on the network and machines.  A change in any one
of these factors could render the service inefficient.

Traditionally, this complexity is addressed by using highly-skilled developers
to build custom systems that are optimized for specific configurations and
objective functions.  Is there a general, principled approach for dealing with
complexity in distributed systems? Hand-tuning the design and configuration of a system
does not scale well with increased complexity and changes in software design, hardware
architecture, deployment environment, and service-level objectives.

We propose to use online learning to cope with this complexity. Specifically, we
view a {\em decision} made by a distributed system in the framework of
contextual online learning with partial feedback. This is a natural fit: we typically
have plenty of context surrounding a decision (which we may not know how to use), the decision
is made online, and we only receive feedback for the decision that was made. Thus, we can
replace a hand-designed policy with a new policy that has been optimized by a contextual
learning system. By supplying rich contexts to this policy, we can capture the complexity of
our environment without having to understand it; by optimizing the policy continuously, we can
can adapt to changes in the environment or service over time.

Machine learning has certainly been used in the past to optimize system decisions. Most
of this use has been restricted to offline settings with full information: for example,
supervised learning is often used to train models on data that has been annotated with
correct labels. Some systems work in an online setting with full information: for example,
a cache that applies an experts learning framework to dynamically switch between existing
(hand-designed) cache eviction policies, by running all policies in parallel and measuring
their performance. Systems that work in an online setting with partial information tend to
be for news or ad recommendation, but examples from systems infrastructure exist: for example,
caching dynamic query content [], tuning parameters of multicore data structures [], allocating
resources to cluster computing tasks [], and allocating servers to web applications [].
In all of these systems, the context used to make a decision, and the
decision itself, are {\em local} to a machine. This is achieved by either devising a decentralized
algorithm in the first place, or giving up on global optimality. Thus we are not aware of any
system that makes ``global'' decisions based on truly distributed architecture or state.

And yet we use hand-designed policies to make these decisions all the time, such as for request
routing, replica placement, replica selection, cloud resource allocation, failure recovery tasks, 
and others. What is it about distributed systems that makes it difficult to apply online learning
at standard decision points? This proposal identifies fundamental challenges posed by distributed
systems that must be analyzed and addressed in order to make online learning a standard tool for
cloud optimization.  These challenges hold even if we assume that the online learning system is
a black-box that delivers optimal decisions in real-time--in other words, they are fundamental
to the way we design distributed systems.  We begin by highlighting some of the unknown factors
that contribute to these challenges, and then list the challenges themselves.

\subsection*{Known unknowns}

Systems often make decisions without complete knowledge of the factors that affect,
or could affect, those decisions.  For example, a caching system evicts items based
on what it believes future requests will look like, without actually knowing the future.
In other cases, incomplete knowledge is due to the absence of data-computation locality.
For example, a load balancer makes decisions based on what it thinks the current load on
machines is, e.g. based on data it collected recently, rather than the actual load at that
very instant in time.

In both of these situations, assumptions are made about the unknowns offline during
\emph{design time}, resulting in different routing and caching algorithms. A change to
the \emph{run-time} environment could easily render these choices suboptimal.

\ignore{Online learning has the potential to make these decisions online, based on current contexts,
and can be nimble in adapting to any changes to the run-time
environment.}

\subsection*{Unknown unknowns}
In some situations, system designers have no way of knowing deployment details
that would impact the system's performance. For example, a system could be built
for external deployment and used by third party customers, or it could be
deployed in a multi-tenant environment where co-located services could impact
its performance.

In these cases, it is near impossible for developers to account for all the
unknown variables in design-time. However, an ML agent is able to make decisions
online based on inputs from the run-time environment.

\ignore{
To be clear, ML has been used in some distributed systems. However, it has not
been used in a general way to make online decisions about core distributed
systems implementation details. We advocate that developers should focus on
high-level objectives of their system, and use online ML to make to compute the
specifics (such as policies for request routing, replica placement, or failure
monitoring) at run-time based on observed metrics.
}

\section{Applying Online Learning to Distributed Systems}

- Framework is to threat ML as a black box. The Decision Service is an exmaple of such a system.
The blackbox takes as input a context, teslls us an action, and then expects us to give it
a reward. Thus it can be placed anywher a decision has to be made, such as which request
to route to, where to place replicas, etc.. Despite the simplicity provided by the ML interface
and we assume the rest of it is solved: it colelcts the data, optimizes a model, and delivers the model
to us quickly (let's even assume the latency of htis loop, which can be minutes, is good enough to adapt
to the rate of change in our environment). Now the remaining piece is to figure out what
to enter as context, how to execut the actions, and how to determine or report rewards.

We argue that there are big challenges even in doing just these things...

Using online ML in distributed systems poses many issues and open questions.
Here are some preliminary questions that we think are worth studying:
- Questions viewed from the lens of characteristics of distributed systems

[merge below above]

\subsection*{Framing}
An online ML loop, like Microsoft's Multi-World Testing Decision Service, views
the external world in terms of \emph{context} (a summarization of the world's
history), \emph{actions} suggested by the agent, and \emph{rewards} collected on
these actions. How can a distributed system's current state and service-level
objectives be translated into context and rewards? In some scenarios, such as
request routing for load balancing, the translation is easy. In others, like
replica placement, the translation is hard.

\subsection*{Locality}
Decisions made by an ML agent in distributed system are likely to rely on input
data collected from diverse and physically distributed components. As a result,
the input to the learning loop might be stale or inconsistent. What could be
done to mitigate those issues while making reasonable real-time decisions?
Additionally, the decisions outputted by the learning agent need to be executed
by diverse and distributed components. How could routine distributed systems
issues such failures and delays in an asynchronous setting affect the learning
system?


\subsection*{Hierarchy}
Distributed systems are often designed in a hierarchical manner, where
subcomponents are monitoring and managing others. These components and layers of
indirection simplify reasoning about the system at large. However, it is not
clear how hierarchy will affect an ML agent.
\begin{itemize}
  \item Will hierarchy aid the learning system via parallelism? Or will it hinder
    it since not all features are visible at every level?
  \item Should learning happen in each component of the system? Or should it be
    centralized?
  \item If learning is happening at multiple levels of the hierarchy, will that
    create interference between the learners? Or will they complement each
    other?
\end{itemize}

- Should feature vectors cut across the levels or not?
- In general should be composable if at each level can define decision and reward,
and exploration randomization is different at each level


\subsection*{Feature selection}
What features of the run-time system are important to the ML agent? How can they
be captured effectively? Can the ML agent learn from incomplete data? For
example, if an ML agent needs to make decisions regarding performance
optimization, would ignoring garbage collection activities or the VMM result in
corrupting the learned model? How sensitive should the learner be?

- quesiton of what features to include is there, but being conservative usually the right
way to go
- bigger quesiton is how ml component deals with omissions and other problems. Separate
into failure sections? Yeah, replace this with ``failures'' and move feature selection stuff
down to last section

\subsection*{Training}
An ML agent can be trained offline with pre-labeled data. However, is this
possible in the context of distributed systems? An online learning systems is
more logical in this context, however, how do we quantify the costs of
exploration vs exploitation of learned strategies?

- Probably omit this, or replace with ``Randomization''. Basically, we already know
that exploratoin-exploitation is the right approach, and that will be encapsulated
by the ML component (its decisions will reflect this, and it will take care of
computing probabilities etc.). But there's an interesting point about inherent randomization
in systems

\section{Expanding the scope}

Two drawbacks: opacity and scope.

Opacity (also, which features matter?)
- We are also pursuing work on cracking models open to see what they are doing inside.
There are different ways to gain insight as to what a model is doing. For example a simple
linear model may learn a vector of weights on the supplied features. These weights can be
studied, and indeed some techniques such as lasso are oftne used to reduce the number of features. However, these
techniques need to be handled carefully. Just becasue a feature has 0 weight doesn't mean it
can be removed: perhaps it is unimportant preceisely because it exists, and its removal all of
asudden changes the weight on other features.


\ignore{
contextual multi-armed bandits. idea is to provide a context, get an action,
and only observe reward for that action. This matches the dist systems world well. It will generate new
policies using ML that we could not come up with by just thinking, and may not
even be able to describe. We argue in favor of opacity and rely instead on the guarantees
provided by the ML optimization.
}


Scope:
The above replaces the policy behind a decision with one backed by an ML learning system.
Indeed systems like hte DS were designed to intervene at the decision making point in a
slim way. The above outlines the issues which tackled would result in ML based policies,
new ones. However, once the above has been mastered, there is the question of whether ML
can influence distributed systems design at a higher level. For example, can it suggest
different mechanisms for achieving a goal, or perhaps a different design of the system in the first place?

\end{document}
